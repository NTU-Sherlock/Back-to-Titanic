# -*- coding: utf-8 -*-
"""Titanic Survival Case.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQAv55gN6sDfZ--OI0vs8NdLCzZHg4Ar
"""

import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
combine = [train_df, test_df]



print(train_df.columns.values)

train_df

train_df.info()
print('_'*40)
test_df.info()

#Total Survived rate

pd.crosstab(index = train_df["Survived"],
                              columns="count")

### How is the Summary Statistics
train_df.describe()

"""### Let's Explore the Features we have!"""

coverage = 891/2224; coverage

sum(train_df['Survived']==1)/len(train_df)

(2224-1502)/2224

sum(train_df['Parch']==0) / len(train_df)

sum(train_df['SibSp']!=0) / len(train_df)

plt.hist(train_df['Fare'])

"""**Does Fare paid matter ?**"""

plt.hist(train_df['Fare'])

"""From the graph above we can see that some transforming the variables is adivsable since the distribution of fare is screwed (to right). We would conduct the this process in data preprocessing."""

plt.scatter(x = train_df['Survived'], y= train_df['Fare'])

"""Interestingly, only those who pay the most ( 512 dollars) is guaranteed to survive. Other premium passangers who pays above $200 has no significant difference."""

plt.hist(train_df['Age'])

grid = sns.FacetGrid(train_df, row='Survived', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()

"""### The distribution of numerical feature values across the training samples

This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.

* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).
* Survived is a categorical feature with 0 or 1 values.
* Around 38% samples survived representative of the actual survival rate at 32%.
* Most passengers (> 75%) did not travel with parents or children.
* Nearly 30% of the passengers had siblings and/or spouse aboard.
* Fares other than $512 has limited implication on passenger survival.

```
# This is formatted as code
```


* Few elderly passengers (<1%) within age range 65-80.
"""

# grid = sns.FacetGrid(train_df, col='Embarked')
grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)
grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')
grid.add_legend()

grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)
grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)
grid.add_legend()

grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend();

# This can show the description categorical data of the df
train_df.describe(include=['O'])

"""What is the distribution of categorical features?

* Names are unique across the dataset (count=unique=891)
* Sex variable as two possible values with 65% male (top=male, freq=577/count=891).
* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.
* Embarked takes three possible values. S port used by most passengers (top=S)
Ticket feature has high ratio (22%) of duplicate values (unique=681).

## Feature Engineering:Title Extracting
How to use the variable 'Name' wisely?
"""

for dataset in combine:
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

pd.crosstab(train_df['Title'], train_df['Sex'])

train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()

for dataset in combine:
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\
 	'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    
train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()

train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()

title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}
for dataset in combine:
    dataset['Title'] = dataset['Title'].map(title_mapping)
    dataset['Title'] = dataset['Title'].fillna(0)

train_df.head()

#Drop unnecessary columns
train_df = train_df.drop(['Name', 'PassengerId'], axis=1)
test_df = test_df.drop(['Name'], axis=1)
combine = [train_df, test_df]
train_df.shape, test_df.shape

for dataset in combine:
    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)

train_df.head()

"""### Engineer on new variable: 
#### Family size
Family members are likely to stay together when catestrophe happen. However, larger family may have higher difficulty and burden to survive in such disaster. We would like to combine SibSp and Parch to see whether this Hypothesis is true.
"""

for dataset in combine:
    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1

train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)

"""Looks like family of four is perfect size to weather the storm! Maybe that's why so many disaster movie are base on a family perspective."""

for dataset in combine:
    dataset['TravelAlone'] = np.where(dataset['FamilySize']==1,1,0)
train_df[['TravelAlone', 'Survived']].groupby(['TravelAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False)

"""Bachelor such like Jack had borned higher risk of death once they step on the ship.

### Data **Preprocessing:** Decision on ** **NA
"""

#NA counts of each variabels
len(train_df) - train_df.count()

print("Before", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)

train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)
test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)
combine = [train_df, test_df]

"After", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape

"""We will drop cabin for the high missing variables are too many, and we would drop ticket since the category are too many to be meaningful.

There are still **177** oberservation need to impute for their **age**, and 2 obs need to be imputed for the **embarked**

---

**Completing a numerical continuous feature**

Imputing with median may sounds easier, but here we would like to try something more reasonable. That's, generate random numbers between mean and standard deviation from the similar group where the objects are alike in class and gender.
"""

grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()

# grid = sns.FacetGrid(train_df, col='Embarked')
grid = sns.FacetGrid(train_df, row='Pclass', col='TravelAlone', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()

"""Since class and gender can hardly tell whether the object is an adult or a child and so define his age, we would consider one more variable "TravelAlone" to help impute age indicate whether the person is or not a kid. (Children would is not likely to board on such a big boat as Titanic alone)"""

guess_ages = np.zeros((2,2,3))
guess_ages

for dataset in combine:
    for i in range(0, 2):
      for k in range(0, 2):
        for j in range(0, 3):
            guess_df = dataset[(dataset['Sex'] == i) & (dataset['TravelAlone'] == k) & \
                                  (dataset['Pclass'] == j+1)]['Age'].dropna()

            age_mean = guess_df.mean()
            age_std = guess_df.std()
            age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,k,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
      for k in range(0, 2):
        for j in range(0, 3):
            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.TravelAlone == k) & (dataset.Pclass == j+1),\
                    'Age'] = guess_ages[i,k,j]

    dataset['Age'] = dataset['Age'].astype(int)

"""#### Interactive Term: Class vs. Age"""

for dataset in combine:
    dataset['Age*Class'] = dataset.Age * dataset.Pclass

train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)

freq_port = train_df.Embarked.dropna().mode()[0]
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)
    
train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)

"""**Make sure there is no more NA**"""

len(test_df) - test_df.count()

for dataset in combine:
    for i in range(0, 2):
      for k in range(0, 2):
        for j in range(0, 3):
            guess_df = dataset[(dataset['Sex'] == i) & (dataset['TravelAlone'] == k) & \
                                  (dataset['Pclass'] == j+1)]['Fare'].dropna()

            fare_mean = guess_df.mean()
            fare_std = guess_df.std()
            fare_guess = rnd.uniform(fare_mean - fare_std, fare_mean + fare_std)

            fare_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_fare[i,k,j] = int( fare_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
      for k in range(0, 2):
        for j in range(0, 3):
            dataset.loc[ (dataset.Fare.isnull()) & (dataset.Sex == i) & (dataset.TravelAlone == k) & (dataset.Pclass == j+1),\
                    'Fare'] = guess_fare[i,k,j]

    #dataset['Fare'] = dataset['Fare'].astype(int)

train_df.head()

"""There is one more observation in test set, Fare, need to be imputed. Here we would apply similiar method as we do on age."""

len(test_df) - test_df.count()

"""**Converting categorical feature into Proper type**

We can now convert the EmbarkedFill feature by creating a new numeric Port feature.
"""

for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

train_df.head()

"""**Feature Transformation for Fare**
There three procedure we can try for feature transformation:
* Standardized the variable, the simplest one but may provide limited change.
* Log transformation could help to deal with outlier 
* Converting a numeric feature into ordinal feature
"""

from sklearn.preprocessing import scale

# Scale the features: X_scaled
fare_scaled = scale(train_df['Fare'])

plt.hist(fare_scaled)

fare_log = np.log(train_df['Fare']+0.1)

plt.hist(fare_log)

"""We can see log transformation is better than normalization. However, could tranforming into ordinal feature fit better?"""

train_df.describe()

train_df['FareBand'] = pd.qcut(train_df['Fare'],10)
train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)

for dataset in combine:
    dataset.loc[(dataset['Fare'] <= 10.5), 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 39.688), 'Fare']   = 1
    dataset.loc[(dataset['Fare'] > 39.688) & (dataset['Fare'] <= 500), 'Fare']   = 2
    dataset.loc[ dataset['Fare'] > 500, 'Fare'] = 3
    dataset['Fare'] = dataset['Fare'].astype(int)

train_df = train_df.drop(['FareBand'], axis=1)
combine = [train_df, test_df]
    
train_df.head(10)

plt.hist(train_df['Fare'])

"""The histgram is not perfert though after transforming numeric fare in to ordinal one.

### Modeling
"""

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

X_Train = train_df.drop("Survived", axis=1)
Y_Train = train_df["Survived"]
X_predict  = test_df.drop("PassengerId", axis=1).copy()
X_Train.shape, Y_Train.shape, X_predict.shape

X_train, X_test, y_train, y_test = train_test_split(X_Train,Y_Train,test_size = 0.2,random_state = 123)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""#### SVM"""

steps = [('SVM', SVC())]

pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'SVM__C':[1, 10, 100],
              'SVM__gamma':[0.1, 0.01]}

# Instantiate the GridSearchCV object: cv
cv = GridSearchCV(pipeline,parameters)

# Fit to the training set
cv.fit(X_train,y_train)

# Predict the labels of the test set: y_pred
y_pred = cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Parameters: {}".format(cv.best_params_))
acc_svc = cv.score(X_test, y_test)

# Support Vector Machines

svc = SVC(C = 10, gamma = 0.01)
svc.fit(X_Train, Y_Train)
svc_Y_pred = svc.predict(X_predict)
svc_Y_pred

"""#### KNN"""

#Tuning parameter K
# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier(n_neighbors=k)

    # Fit the classifier to the training data
    knn.fit(X_train,y_train)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)

    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()

n = 2
knn = KNeighborsClassifier(n_neighbors= n)

# Fit the classifier to the training data
knn.fit(X_train,y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print("Accuracy: {}".format(knn.score(X_test, y_test)))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print("KNN Neighbors = {}".format(n))
acc_knn = knn.score(X_test, y_test)

knn.fit(X_Train,Y_Train)
knn_Y_pred = knn.predict(X_predict)
knn_Y_pred

"""#### Logistic Regression

**Hyperparameters tuning:GridSearch CV**
"""

# Create the hyperparameter grid
c_space = np.logspace(-5, 8, 15)
param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}

# Instantiate the logistic regression classifier: logreg
logreg = LogisticRegression()

# Create train and test sets

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg,param_grid,cv = 5)

# Fit it to the training data
logreg_cv.fit(X_train,y_train)

y_pred = logreg_cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))


# Print the optimal parameters and best score
print("Tuned Logistic Regression Parameter: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Accuracy: {}".format(logreg_cv.best_score_))

from sklearn.feature_selection import f_regression
logreg = LogisticRegression(**logreg_cv.best_params_)

# Fit it to the training data
logreg.fit(X_train,y_train)
y_pred = logreg.predict(X_test)
print(logreg.score(X_test,y_test))
#Coefficient report
coeff_df = pd.DataFrame(train_df.columns.delete(0))
coeff_df.columns = ['Feature']
coeff_df["Correlation"] = pd.Series(logreg.coef_[0])
coeff_df["P-value"] = pd.Series(f_regression(X_train, y_train)[1])
coeff_df.sort_values(by='P-value', ascending=True)

#Comparison without Hyperparameters tuning

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print(logreg.score(X_test, y_test))
acc_log = logreg.score(X_test, y_test)

"""Interestingly, the default set for cost and penalty perform better. Perhaps due to the lack of samples, we can not see a significant benefits for parameters tuning.

Here we will use default set (with C = 1 and penalty = l2) of Logistic Regression instead.
"""

logreg = LogisticRegression()
logreg.fit(X_Train,Y_Train)
logreg_Y_pred = logreg.predict(X_predict)
logreg_Y_pred

"""#### Decision Tree"""

from scipy.stats import randint
from sklearn.model_selection import RandomizedSearchCV

"""**Hyperparameters tuning:RandomizedSearchCV**"""

# Setup the parameters and distributions to sample from: param_dist
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(tree,param_dist , cv=5)

# Fit it to the data
tree_cv.fit(X_train,y_train)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
print("Test Accuracy is {}".format(tree_cv.score(X_test,y_test)))

"""**Hyperparameters tuning:GridSearch CV**"""

# Setup the parameters and distributions to sample from: param_dist
param_grid = {"max_depth": [3, None],
              "max_features": range(1, 9),
              "min_samples_leaf": range(1, 9),
              "criterion": ["gini", "entropy"]}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeClassifier(random_state = 123)

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = GridSearchCV(tree,param_grid, cv=5)

# Fit it to the data
tree_cv.fit(X_train,y_train)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))
print("Test Accuracy is {}".format(tree_cv.score(X_test,y_test)))
acc_decision_tree = tree_cv.score(X_test,y_test)

"""RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time

However, since the Titanic dateset is small, we would aim for higher accuracy and use GridSearchCV
"""

tree = DecisionTreeClassifier(**tree_cv.best_params_)
tree.fit(X_Train,Y_Train)
tree_Y_pred = tree.predict(X_predict)
tree_Y_pred

"""#### Random Forest"""

random_forest = RandomForestClassifier(n_estimators=100,random_state = 123)
random_forest.fit(X_train, y_train)
acc_random_forest = random_forest.score(X_test, y_test)
acc_random_forest

random_forest.fit(X_Train,Y_Train)
rf_Y_pred = tree.predict(X_predict)
rf_Y_pred

"""#### Naive Bayes"""

# Gaussian Naive Bayes

nb = GaussianNB()
nb_cvscores_5 = cross_val_score(nb,X_train,y_train,cv = 5)

#Compute accuracy on the training set
cv_train_accuracy = np.mean(nb_cvscores_5)

# Fit the classifier to the training data
nb.fit(X_train,y_train)    

#Compute accuracy on the testing set
acc_gaussian = nb.score(X_test, y_test)
print(cv_train_accuracy)
print(acc_gaussian)

nb = GaussianNB()
nb.fit(X_Train,Y_Train)
nb_Y_pred = nb.predict(X_predict)
nb_Y_pred

"""### **Model Comparison**"""

models = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
              'Random Forest', 'Naive Bayes','Decision Tree'],
    'Score': [acc_svc, acc_knn, acc_log, 
              acc_random_forest, acc_gaussian, acc_decision_tree]})
models.sort_values(by='Score', ascending=False)

submission = pd.DataFrame({
        "PassengerId": test_df["PassengerId"],
        "Survived": knn_Y_pred
    })
submission.to_csv('knn_submission.csv', index=False)

from google.colab import files
files.download("knn_submission.csv")